{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/OmidGhadami95/Fake_Detection_BERT_Pruning/blob/main/Fake_News_Detection_Without_Pruning.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "q-QLiQ-UY1bH"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from torch.utils.data import TensorDataset, DataLoader, SequentialSampler\n",
        "import torch.nn as nn\n",
        "from sklearn.metrics import classification_report, confusion_matrix\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import re\n",
        "import pycaret\n",
        "import transformers\n",
        "from transformers import AutoModel, BertTokenizerFast\n",
        "#from sklearn.metrics import plot_confusion_matrix\n",
        "from sklearn.metrics import ConfusionMatrixDisplay\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import classification_report\n",
        "from transformers import AutoModel, BertTokenizerFast\n",
        "from tqdm import tqdm\n",
        "# -*- coding: utf-8 -*-\n",
        "\n",
        "# specify GPU\n",
        "device = torch.device(\"cuda\")\n",
        "\n",
        "\n",
        "\"\"\"# Load Dataset\"\"\"\n",
        "\n",
        "def clean_text(text):\n",
        "  # Replace special characters with an empty string\n",
        "  return re.sub(r'[^a-zA-Z0-9\\s]', '', str(text))\n",
        "\n",
        "# Load Dataset\n",
        "true_data = pd.read_csv('a1_True.csv')\n",
        "fake_data = pd.read_csv('a2_Fake.csv')\n",
        "# Generate labels True/Fake under new Target Column in 'true_data' and 'fake_data'\n",
        "true_data['Target'] = ['True']*len(true_data)\n",
        "fake_data['Target'] = ['Fake']*len(fake_data)\n",
        "# Merge 'true_data' and 'fake_data', by random mixing into a single df called 'data'\n",
        "# data = true_data.concat(fake_data).sample(frac=1).reset_index().drop(columns=['index'])\n",
        "data = pd.concat([true_data, fake_data]).sample(frac=1).reset_index(drop=True)\n",
        "\n",
        "# Remove special characters from 'title' and 'text' columns\n",
        "data['title'] = data['title'].apply(clean_text)\n",
        "data['text'] = data['text'].apply(clean_text)\n",
        "\n",
        "print(data.shape)\n",
        "data.head()\n",
        "\n",
        "# Check for null values in the 'text' column\n",
        "null_text_rows = data['text'].isnull().sum()\n",
        "print(f\"Number of null values in 'text' column: {null_text_rows}\")\n",
        "\n",
        "# Drop rows where 'text' column is null\n",
        "data = data.dropna(subset=['text'])\n",
        "\n",
        "# Verify the changes\n",
        "print(f\"Shape of data after dropping rows with null 'text' values: {data.shape}\")\n",
        "\n",
        "# Target column is made of string values True/Fake, let's change it to numbers 0/1 (Fake=1)\n",
        "# data['label'] = pd.get_dummies(data.Target)['Fake']\n",
        "data['label'] = data['Target'].map({'True': 0, 'Fake': 1})\n",
        "\n",
        "data.head()\n",
        "\n",
        "# Checking if our data is well balanced\n",
        "label_size = [data['label'].sum(),len(data['label'])-data['label'].sum()]\n",
        "plt.pie(label_size,explode=[0.1,0.1],colors=['firebrick','navy'],startangle=90,shadow=True,labels=['Fake','True'],autopct='%1.1f%%')\n",
        "\n",
        "\"\"\"# Train-Test Split\"\"\"\n",
        "\n",
        "# Train-Validation-Test set split into 70:15:15 ratio\n",
        "# Train-Temp split\n",
        "train_text, temp_text, train_labels, temp_labels = train_test_split(data['text'], data['label'],\n",
        "                                                                    random_state=2018,\n",
        "                                                                    test_size=0.3,\n",
        "                                                                    stratify=data['Target'])\n",
        "# Validation-Test split\n",
        "val_text, test_text, val_labels, test_labels = train_test_split(temp_text, temp_labels,\n",
        "                                                                random_state=2018,\n",
        "                                                                test_size=0.5,\n",
        "                                                                stratify=temp_labels)\n",
        "\n",
        "\"\"\"# BERT Fine-tuning\"\"\"\n",
        "\n",
        "# Load Pre-trained BERT Model\n",
        "# Load BERT model and tokenizer via HuggingFace Transformers\n",
        "bert = AutoModel.from_pretrained('bert-base-uncased')\n",
        "tokenizer = BertTokenizerFast.from_pretrained('bert-base-uncased')\n",
        "\n",
        "# Plot histogram of the number of words in train data 'text'\n",
        "seq_len = [len(text.split()) for text in train_text]\n",
        "pd.Series(seq_len).hist(bins = 40,color='firebrick')\n",
        "plt.xlabel('Number of Words')\n",
        "plt.ylabel('Number of texts')\n",
        "\n",
        "# Majority of titles above have word length under 250. So, we set max title length as 250\n",
        "MAX_LENGHT = 250\n",
        "# Tokenize and encode sequences in the train set\n",
        "tokens_train = tokenizer.batch_encode_plus(\n",
        "    train_text.tolist(),\n",
        "    max_length = MAX_LENGHT,\n",
        "    pad_to_max_length=True,\n",
        "    truncation=True\n",
        ")\n",
        "# tokenize and encode sequences in the validation set\n",
        "tokens_val = tokenizer.batch_encode_plus(\n",
        "    val_text.tolist(),\n",
        "    max_length = MAX_LENGHT,\n",
        "    pad_to_max_length=True,\n",
        "    truncation=True\n",
        ")\n",
        "# tokenize and encode sequences in the test set\n",
        "tokens_test = tokenizer.batch_encode_plus(\n",
        "    test_text.tolist(),\n",
        "    max_length = MAX_LENGHT,\n",
        "    pad_to_max_length=True,\n",
        "    truncation=True\n",
        ")\n",
        "\n",
        "# Convert lists to tensors\n",
        "train_seq = torch.tensor(tokens_train['input_ids'])\n",
        "train_mask = torch.tensor(tokens_train['attention_mask'])\n",
        "train_y = torch.tensor(train_labels.tolist())\n",
        "\n",
        "val_seq = torch.tensor(tokens_val['input_ids'])\n",
        "val_mask = torch.tensor(tokens_val['attention_mask'])\n",
        "val_y = torch.tensor(val_labels.tolist())\n",
        "\n",
        "test_seq = torch.tensor(tokens_test['input_ids'])\n",
        "test_mask = torch.tensor(tokens_test['attention_mask'])\n",
        "test_y = torch.tensor(test_labels.tolist())\n",
        "\n",
        "# Data Loader structure definition\n",
        "from torch.utils.data import TensorDataset, DataLoader, RandomSampler, SequentialSampler\n",
        "batch_size = 32                                               #define a batch size\n",
        "train_data = TensorDataset(train_seq, train_mask, train_y)    # wrap tensors\n",
        "train_sampler = RandomSampler(train_data)                     # sampler for sampling the data during training\n",
        "train_dataloader = DataLoader(train_data, sampler=train_sampler, batch_size=batch_size)\n",
        "                                                              # dataLoader for train set\n",
        "val_data = TensorDataset(val_seq, val_mask, val_y)            # wrap tensors\n",
        "val_sampler = SequentialSampler(val_data)                     # sampler for sampling the data during training\n",
        "val_dataloader = DataLoader(val_data, sampler = val_sampler, batch_size=batch_size)\n",
        "                                                              # dataLoader for validation set\n",
        "\n",
        "\n",
        "# Load the pre-trained BERT model and tokenizer\n",
        "bert = AutoModel.from_pretrained('bert-base-uncased')\n",
        "tokenizer = BertTokenizerFast.from_pretrained('bert-base-uncased')\n",
        "\n",
        "# Define the BERT architecture\n",
        "class BERT_Arch(nn.Module):\n",
        "    def __init__(self, bert):\n",
        "        super(BERT_Arch, self).__init__()\n",
        "        self.bert = bert\n",
        "        self.dropout = nn.Dropout(0.1)\n",
        "        self.relu = nn.ReLU()\n",
        "        self.fc1 = nn.Linear(768, 512)\n",
        "        self.fc2 = nn.Linear(512, 2)\n",
        "        self.softmax = nn.LogSoftmax(dim=1)\n",
        "\n",
        "    def forward(self, sent_id, mask):\n",
        "        cls_hs = self.bert(sent_id, attention_mask=mask)['pooler_output']\n",
        "        x = self.fc1(cls_hs)\n",
        "        x = self.relu(x)\n",
        "        x = self.dropout(x)\n",
        "        x = self.fc2(x)\n",
        "        x = self.softmax(x)\n",
        "        return x\n",
        "\n",
        "# Initialize the model\n",
        "model = BERT_Arch(bert)\n",
        "\n",
        "# Load the trained model weights\n",
        "model.load_state_dict(torch.load('c3_new_model_weights.pt'))\n",
        "model.eval()\n",
        "\n",
        "# Tokenize and encode the validation data\n",
        "MAX_LENGTH = 250\n",
        "tokens_val = tokenizer.batch_encode_plus(\n",
        "    val_text.tolist(),\n",
        "    max_length=MAX_LENGTH,\n",
        "    pad_to_max_length=True,\n",
        "    truncation=True\n",
        ")\n",
        "\n",
        "# Convert to tensors\n",
        "val_seq = torch.tensor(tokens_val['input_ids'])\n",
        "val_mask = torch.tensor(tokens_val['attention_mask'])\n",
        "val_y = torch.tensor(val_labels.tolist())\n",
        "\n",
        "# Create DataLoader\n",
        "batch_size = 32\n",
        "val_data = TensorDataset(val_seq, val_mask, val_y)\n",
        "val_sampler = SequentialSampler(val_data)\n",
        "val_dataloader = DataLoader(val_data, sampler=val_sampler, batch_size=batch_size)\n",
        "\n",
        "# Evaluation function with progress tracking\n",
        "def evaluate_model(model, dataloader):\n",
        "    model.eval()\n",
        "    predictions = []\n",
        "    true_labels = []\n",
        "\n",
        "    progress_bar = tqdm(dataloader, desc=\"Evaluating\", total=len(dataloader))\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for batch in progress_bar:\n",
        "            sent_id, mask, labels = batch\n",
        "            outputs = model(sent_id, mask)\n",
        "            _, preds = torch.max(outputs, dim=1)\n",
        "            predictions.extend(preds.cpu().tolist())\n",
        "            true_labels.extend(labels.cpu().tolist())\n",
        "\n",
        "            progress_bar.set_postfix({\"Batch\": f\"{progress_bar.n}/{len(dataloader)}\"})\n",
        "\n",
        "    return predictions, true_labels\n",
        "\n",
        "print(\"Starting evaluation...\")\n",
        "predictions, true_labels = evaluate_model(model, val_dataloader)\n",
        "print(\"Evaluation completed.\")\n",
        "\n",
        "# Generate classification report\n",
        "report = classification_report(true_labels, predictions, target_names=['True', 'Fake'])\n",
        "print(report)\n",
        "\n",
        "# Generate confusion matrix\n",
        "cm = confusion_matrix(true_labels, predictions)\n",
        "\n",
        "# Plot confusion matrix\n",
        "plt.figure(figsize=(10, 8))\n",
        "sns.heatmap(cm, annot=True, fmt='d', cmap='Blues')\n",
        "plt.title('Confusion Matrix')\n",
        "plt.xlabel('Predicted')\n",
        "plt.ylabel('True')\n",
        "plt.savefig('confusion_matrix.pdf')\n",
        "plt.close()\n",
        "\n",
        "# Plot classification report\n",
        "report_dict = classification_report(true_labels, predictions, target_names=['True', 'Fake'], output_dict=True)\n",
        "df_report = pd.DataFrame(report_dict).transpose()\n",
        "\n",
        "plt.figure(figsize=(12, 8))\n",
        "sns.heatmap(df_report.iloc[:-1, :-1].astype(float), annot=True, cmap='YlGnBu')\n",
        "plt.title('Classification Report Heatmap')\n",
        "plt.savefig('classification_report.pdf')\n",
        "plt.close()\n",
        "\n",
        "print(\"Evaluation completed. Check 'confusion_matrix.pdf' and 'classification_report.pdf' for visualizations.\")\n",
        "import torch\n",
        "from transformers import AutoModel, BertTokenizerFast\n",
        "import torch.nn as nn\n",
        "\n",
        "# Load BERT model and tokenizer\n",
        "bert = AutoModel.from_pretrained('bert-base-uncased')\n",
        "tokenizer = BertTokenizerFast.from_pretrained('bert-base-uncased')"
      ]
    }
  ]
}